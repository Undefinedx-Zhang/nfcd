# NFCD: Normalizing Flow for Change Detection

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.7+](https://img.shields.io/badge/python-3.7+-blue.svg)](https://www.python.org/downloads/)
[![PyTorch](https://img.shields.io/badge/PyTorch-1.1+-ee4c2c.svg)](https://pytorch.org/)

## ğŸ“‹ Description

This project implements a **semi-supervised change detection framework** for remote sensing images using **Normalizing Flow (NF)** to generate pseudo labels. The framework employs a three-stage training strategy to effectively leverage both labeled and unlabeled data for improved change detection performance.

### Key Features

- ğŸ”¥ **Three-Stage Training Pipeline**: Progressive training strategy for optimal performance
- ğŸ¯ **Normalizing Flow Integration**: Generates high-quality pseudo labels for unlabeled data
- ğŸ—ï¸ **Multiple Backbone Support**: ResNet50, ResNet101, HRNet architectures
- ğŸ“Š **Multiple Dataset Support**: CDD, LEVIR-CD, WHU-CD datasets
- ğŸ”„ **Semi-Supervised Learning**: Efficient utilization of limited labeled data
- ğŸ“ˆ **Consistency Regularization**: Feature alignment loss for robust predictions

## ğŸ—‚ï¸ Project Structure

```
nfcd/
â”œâ”€â”€ base/                   # Base classes for datasets, models, and trainers
â”‚   â”œâ”€â”€ base_dataloader.py
â”‚   â”œâ”€â”€ base_dataset.py
â”‚   â”œâ”€â”€ base_model.py
â”‚   â””â”€â”€ base_trainer.py
â”œâ”€â”€ configs/               # Configuration files for different datasets
â”‚   â”œâ”€â”€ config_CDD.json
â”‚   â”œâ”€â”€ config_LEVIR.json
â”‚   â””â”€â”€ config_WHU.json
â”œâ”€â”€ dataloaders/           # Data loading and preprocessing
â”‚   â”œâ”€â”€ CDDataset.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ models/                # Model architectures
â”‚   â”œâ”€â”€ backbones/        # Backbone networks (ResNet, HRNet)
â”‚   â”œâ”€â”€ decoder.py        # Decoder modules
â”‚   â”œâ”€â”€ encoder.py        # Encoder modules
â”‚   â”œâ”€â”€ nf.py            # Normalizing Flow implementation
â”‚   â”œâ”€â”€ NF_ResNet50_CD.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ utils/                 # Utility functions
â”‚   â”œâ”€â”€ helpers.py
â”‚   â”œâ”€â”€ losses.py
â”‚   â”œâ”€â”€ metrics.py
â”‚   â”œâ”€â”€ visualize.py
â”‚   â””â”€â”€ ...
â”œâ”€â”€ train.py              # Training script
â”œâ”€â”€ trainer.py            # Trainer implementation
â”œâ”€â”€ inference.py          # Inference script
â”œâ”€â”€ visual.py             # Visualization tools
â””â”€â”€ requirements.txt      # Python dependencies
```

## ğŸ“¦ Dataset Information

### Supported Datasets

1. **CDD (Change Detection Dataset)**
2. **LEVIR-CD** (LEVIR Change Detection Dataset)
3. **WHU-CD** (WHU Building Change Detection Dataset)

### Dataset Structure

Organize your dataset as follows:

```
DATA/
â”œâ”€â”€ CDD/                  # or LEVIR/WHU
â”‚   â”œâ”€â”€ A/               # Pre-change images
â”‚   â”œâ”€â”€ B/               # Post-change images
â”‚   â”œâ”€â”€ label/           # Ground truth labels
â”‚   â””â”€â”€ list/            # Train/val/test split files
â”‚       â”œâ”€â”€ train_supervised.txt
â”‚       â”œâ”€â”€ train_unsupervised.txt
â”‚       â”œâ”€â”€ val.txt
â”‚       â””â”€â”€ test.txt
```

## ğŸ› ï¸ Requirements

### Dependencies

```bash
Python >= 3.7
PyTorch >= 1.1.0
torchvision
numpy >= 1.16.3
matplotlib >= 3.1.1
opencv-python >= 4.1.1.26
tensorboard
tqdm >= 4.38.0
scikit-image >= 0.15.0
scipy
FrEIA  # For Normalizing Flow
```

### Installation

1. **Clone the repository**
```bash
git clone https://github.com/yourusername/nfcd.git
cd nfcd
```

2. **Create a virtual environment (recommended)**
```bash
conda create -n nfcd python=3.8
conda activate nfcd
```

3. **Install dependencies**
```bash
pip install -r requirements.txt
```

4. **Install FrEIA (Normalizing Flow library)**
```bash
pip install git+https://github.com/VLL-HD/FrEIA.git
```

## ğŸš€ Usage Instructions

### 1. Configuration

Edit the configuration file for your dataset (e.g., `configs/config_CDD.json`):

```json
{
  "name": "NFCD",
  "percent": 5,                    // Percentage of labeled data (5%, 10%, 20%)
  "model": {
    "backbone": "ResNet50",        // Backbone: ResNet50, ResNet101, HRNet (NF kept for legacy configs)
    "confidence_thr": 0.95,        // Confidence threshold for pseudo labels
    "nf_weight": 0.7               // Weight for NF loss
  },
  "train_supervised": {
    "data_dir": "/path/to/dataset",
    "batch_size": 4,
    "crop_size": 256
  }
}
```

### 2. Training

The training process consists of three stages:

#### Stage 1: Base Model Training
```bash
python train.py \
    --config configs/config_CDD.json \
    --gpu 0 \
    --aug_type all
```

#### Stage 2: Normalizing Flow Training
The framework automatically trains the Normalizing Flow model after Stage 1.

#### Stage 3: Pseudo Label Refinement
The model generates and refines pseudo labels using the trained NF model.

**Control training stages** by modifying the `process` parameter in `config.json`:
- `[1]`: Only Stage 1
- `[1, 2]`: Stages 1 and 2
- `[1, 2, 3, 4]`: All stages (full pipeline)

### 3. Inference

Run inference on test data:

```bash
python inference.py \
    --config configs/config_CDD.json \
    --model /path/to/best_model.pth \
    --Dataset_Path /path/to/test/dataset \
    --save
```

### 4. Visualization

Visualize predictions:

```bash
python visual.py \
    --config configs/config_CDD.json \
    --model /path/to/best_model.pth \
    --Dataset_Path /path/to/dataset \
    --method NF
```

## ğŸ§ª Methodology

### Three-Stage Training Strategy

#### **Stage 1: Supervised Baseline Training**
- Train the base change detection model using labeled data
- Apply consistency regularization between weak and strong augmentations
- Loss: `L_total = L_supervised + L_consistency + L_alignment`

#### **Stage 2: Normalizing Flow Training**
- Freeze the base model
- Train Normalizing Flow decoders on multi-scale features
- Learn probability distributions of unchanged pixels
- Generate anomaly scores for change detection

#### **Stage 3: Pseudo Label Generation & Refinement**
- Generate pseudo labels using trained NF model
- Apply confidence-based filtering
- Refine predictions using connected component analysis
- Fine-tune the model with combined labeled and pseudo-labeled data


## ğŸ“ Model Checkpoints

Trained models are saved in:
```
outputs/
â”œâ”€â”€ DATASET_NAME/
â”‚   â”œâ”€â”€ stage1/
â”‚   â”‚   â””â”€â”€ best_model_thr-0.95.pth
â”‚   â”œâ”€â”€ stage2/
â”‚   â”‚   â””â”€â”€ nf/best_model_nf_decoders.pth
â”‚   â”œâ”€â”€ fake_labels/
â”‚   â”‚   â”œâ”€â”€ Label_batch_0.pt
â”‚   â”‚   â”œâ”€â”€ noLabel_batch_0.pt
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ stage3/
â”‚       â””â”€â”€ weightXX/
â”‚           â””â”€â”€ best_model.pth
```
Pseudo labels are shared across all weight settings and live directly under `fake_labels`; only the stage3 checkpoints remain weight-specific.


## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.
