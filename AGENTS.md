# Repository Guidelines

## Project Structure & Module Organization
NFCD centers on Python packages in `base/`, `dataloaders/`, and `models/`. Extend the abstractions in `base/base_model.py`, `base/base_dataloader.py`, and `base/base_trainer.py` instead of duplicating trainer logic, and keep dataset-specific parsing inside `dataloaders/CDDataset.py`. Model variants (ResNet, HRNet, NF decoders) live under `models/`, while experiment entry points are `train.py`, `trainer.py`, `inference.py`, and `visual.py`. JSON configs in `configs/` set dataset paths, backbone choices, and experiment names that drive the directory layout under `outputs/<dataset>/<experiment>/stage*`. Keep generated checkpoints, pseudo labels, and plots inside `outputs/` so the repository tree stays clean; the vendorized Normalizing Flow blocks live in `FrEIA/`, and helpers (losses, metrics, visualization) are consolidated under `utils/`.

## Build, Test, and Development Commands
`python -m pip install -r requirements.txt` installs the shared scientific stack (install a CUDA-matched PyTorch beforehand). `python -m pip install git+https://github.com/VLL-HD/FrEIA.git` is required once for flow layers. Launch a training run with `python train.py --config configs/config_CDD.json --gpu 0 --aug_type all`, which triggers the staged Trainer and writes checkpoints to the config-defined `trainer.save_dir`. Evaluate a checkpoint via `python inference.py --config configs/config_CDD.json --model outputs/CDD/<exp>/stage3_nf/best_model_thr-0.95.pth --Dataset_Path DATA/CDD`, and produce qualitative overlays with `python visual.py --config configs/config_CDD.json --split val --output_dir outputs/vis/cdd`.

## Coding Style & Naming Conventions
Stick to Python 3.7+ and PEP 8: 4-space indentation, <=100-character lines, and descriptive snake_case for functions/variables. Class names remain PascalCase (`CDDataset`, `NF_ResNet50_CD`), and config keys stay lowercase with underscores. Favor explicit type hints for new public functions, keep deterministic seeds where possible, and centralize dataset-specific constants inside the JSON configs instead of embedding hard-coded paths in scripts. Shared math, visualization, and loss implementations belong in `utils/` so orchestration scripts stay slim.

## Testing Guidelines
There is no standalone unit-test harness, so rely on functional checks. For every change, run a minimal experiment (e.g., set `"percent": 5` in the config) and verify stage-1 loss curves before letting the pipeline advance. Re-run `python inference.py` on the smallest dataset split and record IoU/F1 deltas; when editing dataloaders, temporarily use `--aug_type identity` to guarantee paired crops remain aligned. Capture metrics, console samples, and qualitative findings in `outputs/<dataset>/<experiment>/notes.md` (create it if missing) so reviewers can replay the run.

## Commit & Pull Request Guidelines
Write concise, imperative commits that describe behavior, e.g., `feat: add HRNet pseudo-label alignment`, and bundle code plus its config changes together. Never commit raw datasets or large checkpointsâ€”reference their locations instead. Pull requests should outline the dataset/config used, summarize metric changes, and attach at least one visualization or tensorboard screenshot whenever model behavior changes. Link related issues (or provide context when none exists) and highlight backward-incompatible edits to config schemas, output paths, or Trainer defaults to keep downstream automation stable.
